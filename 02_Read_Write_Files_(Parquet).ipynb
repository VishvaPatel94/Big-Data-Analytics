{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 Read-Write Files (Parquet).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNye1AroNWyU52HzmoD6THG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/revendrat/Big-Data-Analytics/blob/main/02_Read_Write_Files_(Parquet).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal:\n",
        "1. Create an array with 100 numbers, from 0 to 99\n",
        "2. Write this data on Parquet format \n",
        "3. Read a Parquet format file\n",
        "4. Parquet File read/write operations are performed using the functions provided by the pyarrow.parquet module "
      ],
      "metadata": {
        "id": "J4tDihY0M7Aj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMCgREUoMZrz",
        "outputId": "2b4121df-1d64-48ef-bf50-608bbe627838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 .. 99\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pyarrow as pa\n",
        "\n",
        "arr = pa.array(np.arange(100))\n",
        "\n",
        "print(f\"{arr[0]} .. {arr[-1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.\n",
        "* Read more on Parquet here: https://parquet.apache.org/ <br/>\n",
        "\n",
        "1. Create a pyarrow.Table from arr\n",
        "2. Write an pyarrow.Table  to a Parquet file \n",
        "\n",
        "* Note that a table of a single column is created and then be written to a Parquet file."
      ],
      "metadata": {
        "id": "A23WrEsBN_uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table = pa.Table.from_arrays([arr], names=[\"col1\"])"
      ],
      "metadata": {
        "id": "cYZWWzU0MyaY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the table is created, the next step is write it to parquet format"
      ],
      "metadata": {
        "id": "tR9ARv8tMzQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the data from table to example.parquet\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "pq.write_table(table, \"example.parquet\", compression=None)"
      ],
      "metadata": {
        "id": "9yq2LXD1PMVG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the file using ls command\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_G6vtTPPO2X",
        "outputId": "44d0df9c-9723-4266-9a26-6ea8a106255a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example.parquet  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the data from example.parquet file to nums100"
      ],
      "metadata": {
        "id": "P6T9UX0iQ4fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data from example.parquet to nums100\n",
        "nums100 = pq.read_table(\"example.parquet\")\n",
        "print(nums100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5jLVOdGPLRn",
        "outputId": "8529d5ea-782c-4e97-9e7f-fb41843f55e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pyarrow.Table\n",
            "col1: int64\n",
            "----\n",
            "col1: [[0,1,2,3,4,5,6,7,8,9,...,90,91,92,93,94,95,96,97,98,99]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading a subset of Parquet data\n",
        "* Use filters and columns arguments to restrict which Columns and Rows to read from a Parquet file with pyarrow.parquet.read_table()\n",
        "* Read values between 5 and 10 from example.parquet file"
      ],
      "metadata": {
        "id": "judn6vRbSm2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums5_10 = pq.read_table(\"example.parquet\",\n",
        "                      columns=[\"col1\"],\n",
        "                      filters=[\n",
        "                          (\"col1\", \">\", 5),\n",
        "                          (\"col1\", \"<\", 10),\n",
        "                      ])\n",
        "print(nums5_10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e3e5CQkQzJ9",
        "outputId": "445d1823-4064-471e-fb69-a08b4a17f2e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pyarrow.Table\n",
            "col1: int64\n",
            "----\n",
            "col1: [[6,7,8,9]]\n"
          ]
        }
      ]
    }
  ]
}